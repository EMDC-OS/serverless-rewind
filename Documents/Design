[General Execution Process (OpenWhisk-based)]
1. User: Executes a function and sends the request to OpenWhisk.
2. OpenWhisk:
 - Receives the request via the gateway.
 - Passes the request through internal components to the invoker.
 - The invoker executes a Docker container, starting the function container (docker run ...).
 - During the container creation process, the function source code is placed inside the container (usually under /actionProxy or /action).
3. Docker:
 - Retrieves container information from OpenWhisk.
 - Executes a proxy process within the container to receive function execution details (described in more detail in the baseline case below).
 - OpenWhisk sends messages to the proxy process for function execution (in the experiment, communication was based on a web server). The container executes the function with parameters from the received message and sends the result back to OpenWhisk.
4. OpenWhisk: Processes the received result according to the request (e.g., delivering it to the user, updating a database, etc.).


[Container Process Execution in Baseline and REWIND]
1. During the proxy process execution, an additional process is created to execute the function (i.e., both the proxy and function process exist simultaneously before executing the core logic of the function).
2. In the function execution phase, a loop listens for messages from the proxy, triggering the execution of the function’s core logic when a message is received.
- Refer to Fig. 4 in the paper for pseudocode, which explains checkpoint() and rewind().
3. Unlike fork-based techniques using fork-exec, this approach avoids overhead from process creation, language runtime initialization, and library loading.
- However, compared to the fork-based technique that employs On-demand-fork for executing the function’s core logic, only the process creation overhead is reduced.


[Private Data Removal]
1. Memory
 - Use a buddy page table (BPT) copied from the last-level page table (LPT).
   > LPT size: 4K → 8K.
   > Upper 4K stores PTE information at the checkpoint() point; lower 4K is used as usual.
 - Apply write protection during checkpoint() (track changes to the previously used PTE and pages via CoW).
 - Read fault: When a new PTE is created, write it to the PT and copy the same value to the BPT.
 - Write fault:
   > For anonymous pages, set the erase flag in the PTE and copy the same value to the BPT.
   > For file-backed pages:
   > Shared: same as read fault.
   > Private: Write the new PTE for the page to the PT and record the page cache PTE in the BPT as read-only.
 - CoW (Write-protected fault): Allocate a new page and write the PTE only to the PT (as the previous PTE will be recorded in the BPT).
 - Rewind:
   > Zero the page if the erase flag is set.
   > Copy the page content from the page mapped in the BPT to the page mapped in the PT if their writable flags differ (BPT is read-only, PT is writable).
2. Memory Optimization
 - Reuse VMA:
   > Anonymous VMAs and VMAs with non-fixed addresses are assumed to store temporary data for reuse in subsequent executions.
3. File
 - After container process initialization (just before core logic execution in the proxy), create a snapshot of the filesystem used by the container.
   > Using OverlayFS2, only the modified/added file information relative to the base image is copied.
 - During rewind, restore or remove modified/created files based on the snapshot and container’s base image.
4. Process Information
 - Remove child processes and threads created after the checkpoint.
 - Restore file descriptors (network sockets can be addressed as they share file descriptor structures).


[Optimization - Function Code Refactoring]
1. If data used by the function does not contain private information (e.g., authentication info for shared network storage, dynamically loaded libraries), define it globally to avoid redundancy.


[Experiments]
1. Server: Dell R740 HPC1 server (refer to server files and experiment scripts for core isolation and other specifications).
2. Latency: Measured from when the container process receives a request for core logic execution to when the next execution is ready.
 - For REWIND, calculate based on the time when rewind() completes.
3. Throughput: Measured as the time taken to process 20 requests simultaneously from 12 users.
4. Memory Overhead: Measured using peak RSS memory (verified through /proc/vmstat).
5. Other analysis metrics were extracted alongside latency measurements (some required additional options).
